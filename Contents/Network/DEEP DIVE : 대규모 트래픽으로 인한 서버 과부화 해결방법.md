## 대규모트래픽 처리의 중요성

웹 서비스를 다루는데있어 트래픽을 처리하는 구조를 설계하고 적용하는 것은 개발자의 필수 역량이다.  
특히 서비스의 규모가 커질수록 개발자가 의도한 대로 프로그램이 작동하지않는 경우가 발생한다.  
아무리 뛰어난 성능을 가진 서버라고해도 모든 트래픽을 감당할수는 없으므로 서비스의 안정적인 구동과 만족도높은 고객경험을 제공하기위해 대용량 트래픽을 다루는기술을 학습하고 대비해야한다.  
> ex} 네이버페이, 토스페이, 카카오페이 처럼 경쟁사가 많을수록 서버가 한번터져서 사용자의 요청을 제대로소화해내지 못하면 사용자가 불쾌함과 불편함을 느끼게되고 
사용자가 영구히 이탈해버린다면 큰 손실이 발생한다. 


## 서버가 터지는 이유  
결국 서버도 외부로부터 요청을 받아 처리해주고 응답을 주는 프로그램이 돌아가는 컴퓨터이기때문에 처리속도와 한계가 CPU, 메모리, 저장장치에 영향을 받는다.
사용자가 많아질수록 서버에는 많은 HTTP request가 발생하고 request가 많아졌다는것은 트래픽이 높아졌다는 의미이다.  
Request는 Queue를 통하여 Thread pool에 할당하게되는데 Thread pool size를 초과하는 요청은 큐에서 대기한다. Thread의 개수는 무한하지 않으므로 시스템에 할당된 성능에따라 제한된다.  
따라서 Thread pool size를 초과하는 대량의 트래픽이 지속적으로 발생하면 서버의 지연시간은 기하급수적으로 증가하게된다. 이와같은 현상을 Thread pool hell이라고 한다.

![image](https://github.com/NoRuTnT/practice/assets/114069644/5beb2553-302e-406d-a354-0628cac2a5f1)  

### 큐 오버플로우  
아래는 Tomcat의 요청 수락/처리와 관련된 그림이다.
> ![image](https://github.com/NoRuTnT/practice/assets/114069644/4a49195a-5ecd-4836-bde5-85625a6a3dd2)  
> maxThreads : 동시에 일하는 스레드의 최대 개수이다.  
> maxConnections : 하나의 Tomcat 인스턴스가 동시에 유지할 수 있는 Connection의 최대 개수이다.  
> acceptCount : maxConnections를 초과하는 요청에 대한 대기큐의 크기이다.

maxThreads는 일하는 놈이 몇이나 있는가  
maxConnections는 받아서 처리를 할 예정이지만 아직 처리되지 않은 요청들의 큐( Keep Alive를 통해 일부러 연결을 유지시킨 요청도 포함)  
acceptCount는 maxConnection의 개수를 넘어가는 추가 요청들을 몇 개까지 가지고 있을것인가..  

### 타임아웃
요청에는 대개 타임아웃이 설정되어 있다. 보낸 요청이 처리될 때까지 언제까지고 기다릴 수는 없으므로 일정 시간이 지나면 실패로 간주해버리는 것인데, 서버가 다른 요청들을 처리하느라고 바빠서 그 이후에 들어온 요청들을 타임아웃이 걸릴 때까지 처리하지 못 할 경우, 사용자 입장에서는 역시 서버가 터졌나 싶은 응답을 받게 된다.  

## 해결방안
서버가 터지는 이유는 결국 아직 처리되지 못한 요청이 쌓이고 쌓이고 쌓여서 그렇게 되는 것이므로, 단순히 생각해 요청을 충분히 빠르게 처리하면 된다.  
서버 리소스부족문제는 보통 2가지 방법으로 해결책이 나뉜다.

### (vertical) scale-up
Scale Up 방식은 말그대로 고성능 CPU, 메모리 확장, SSD 등 서버의 스펙을 높이는 수직 확장 방식.
그중에서도 중요한것은 기억장치의 처리속도이다. 예외로 머신러닝같은 모델트레이닝같은경우는 어마어마한양의 연산이 들어가서 cpu가 중요할수도있지만 해당작업은 특별한서비스가아닌이상 드물고 대규모트래픽과는 별 연관이없다.  
그렇기때문에 대규모 트래픽 처리를 위한 scale-up을 고려하는 경우 기억장치의 처리 속도 업그레이드를 고려해야 한다.

### (horizontal) scale out
Scale out 방식은 여러대의 서버를 구축하여 서버 한대가 처리하는 용량을 줄임으로서 전체적인성능을 중가시키는 방법이다.  
한대가 부담해야했던 서버장애 리스크를 줄일수있고 가성비적인측면에서도 비교우위에있어 scale up보다 효율적인 리소스 부족 해결방식이다.  
모든요청이 동일한 처리 시간을 요구하지는 않기 때문에 어떤 요청은 보다 많은 처리 시간이 필요하고, 어떤 요청은 그렇지 않을 것이다.  
이때, 특정 서버에 장시간의 처리가 필요한 요청이 쏠리면 해당 서버만 터지게 될 것이고, 운 없게도 해당 서버로 요청을 보내게 된 사용자들은 불편하고 불쾌한 경험을 하게 될 것이다.  
이런 문제를 해결하기 위한 기술이 로드밸런싱이다.  

### 로드밸런싱
분산시스템을 구축한다고했을때 중요해지는것은 로드밸런싱이다.  
![image](https://github.com/NoRuTnT/practice/assets/114069644/8be0886d-1c38-4d38-a712-f6cc14eaf21c)  
서버를 여러대를 가지고있으니까 들어오는 트래픽을 누구에게보낼지 분산시켜줄 모듈이 필요한데 그것을 로드밸런서라고 부른다.  

### 로드밸런서의 분산전략
#### Round Robin  
여러대의 서버가있을때 들어오는 리퀘스트를 서버에 순차적으로 나누어준다(Like 수건돌리기)  
이런방식은 트래픽을 모든서버가 골고루 분산해서 가져갈수있는 간단한 알고리즘  
![image](https://github.com/NoRuTnT/practice/assets/114069644/a7e47b9f-8f83-4700-abc8-fbaa604e76bf)  

#### Random Select
리퀘스트가들어왔을때 그냥 아무서버를 찍어서 그서버에 리퀘스트를 보내는방식.  
![image](https://github.com/NoRuTnT/practice/assets/114069644/81b5e23c-c1aa-4b54-8b72-f312fea9d852)  

#### Least Connection
서버가 내가 얼마만큼의 트래픽을 감당하고 있고 얼마만큼의 커넥션을 맺고있는지 역으로 알려준다.  
로드 밸런서입장에서 이정보들을 가지고 판단을 한다. Adaptive하게 판단  
![image](https://github.com/NoRuTnT/practice/assets/114069644/38b4d3a2-a966-428a-8db2-a1d329c78665)  


